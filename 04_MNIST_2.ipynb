{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"gpuType":"T4","authorship_tag":"ABX9TyP/u4cg/mwnS/1TpKDUWSaT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AwErIR3bzMUh"},"outputs":[],"source":["!pip install -Uqq fastbook ipywidgets nbdev\n","\n","import fastbook\n","fastbook.setup_book()\n","from fastbook import *\n","from fastai.vision.widgets import *\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","# your drive home will be at /content/gdrive/MyDrive/\n","# your notebooks might be at /content/gdrive/MyDrive/Colab Notebooks/\n","BASE_DIR = \"/content/gdrive/MyDrive/Colab Notebooks/\""]},{"cell_type":"code","source":["# https://wandb.ai/wandb/common-ml-errors/reports/How-To-Use-GPU-with-PyTorch---VmlldzozMzAxMDk\n","# torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"KdaFgGynomM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = untar_data(URLs.MNIST_SAMPLE)\n","\n","# Convert MNIST data in folders to the format we need\n","# 1. Open each image file as an Image object and convert it to tensors\n","# 2. Convert a list of 2D tensors to a 3D tensor using stack(), normalise\n","#     by converting each value to a float and dividing by max pixel value 255\n","# 3. Concatenate all the tensors of 3s and 7s together and make 28x28 arrays\n","#     into a 28*28 length list\n","# 4. Creatae the labels such that wew have 1 for image '3' and 0 for image '7'\n","\n","def filesToStackedTensors(d1, d2):\n","  t = [tensor(Image.open(f)).to(device) for f in (path/d1/d2).ls().sorted()]\n","  return torch.stack(t, 0).float()/255\n","\n","def x_y(stacked_x, stacked_y):\n","  x = torch.cat([stacked_x, stacked_y]).view(-1, 28*28)\n","  y = tensor([1]*len(stacked_x) + [0]*len(stacked_y)).unsqueeze(1).to(device)\n","  return (x, y)"],"metadata":{"id":"vmunicOYzUtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ??torch.cat\n","# ??torch.view\n","# ??torch.unsqueeze\n","\n","a = torch.ones(4, 1)\n","b = torch.full((1, 4, 3), 3)\n","# a, b, torch.cat([a, b]), torch.cat([a, b]).view(-1, 12)\n","\n","# valid values for unsqueeze in this example: -2 to 1\n","# tensor([1]*13 + [0]*4).unsqueeze(1)\n","# tensor([1]*13 + [0]*4).unsqueeze(0)\n","\n","# ??torch.sum\n","a, 1 - a.mean()"],"metadata":{"id":"xJVei1jB0I0Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x, train_y = x_y(filesToStackedTensors('train', '3'), \\\n","                       filesToStackedTensors('train', '7'))\n","valid_x, valid_y = x_y(filesToStackedTensors('valid', '3'), \\\n","                       filesToStackedTensors('valid', '7'))\n","# train_x.shape, train_y.shape, valid_x.shape, valid_y.shape\n","# print(f'x cuda? {train_x.is_cuda}')\n","# print(f'y cuda? {train_y.is_cuda}')"],"metadata":{"id":"8MifxbiyzqbY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predictions(x, w, b):\n","  print(f'w: {w[0:1, 0:1]}')\n","  r = torch.matmul(x, w) + b\n","  print(f'r.grad_fn: {r.grad_fn}')\n","  # print(f'r shape: {r.shape}, r: {r[0:4, 0:4]}')\n","  sums = (r.sum(1)).unsqueeze(1)\n","  print(f'sums.grad_fn: {sums.grad_fn}')\n","  # print(f'sums shape: {sums.shape}, sums: {sums[0:4]}')\n","  sig = torch.sigmoid(sums)\n","  print(f'sig.grad_fn: {sig.grad_fn}')\n","  # print(f'sig shape: {sig.shape}, sig: {sig[0:4]}')\n","  # print(f'sig: {sig[0:4]}, sig.sum: {sig.sum()}')\n","  return sums\n","\n","def loss(preds, y):\n","  # print(f'preds: {preds[0:5]}')\n","  # print(f'labels: {y[0:5]}')\n","  # print(f'x shape: {preds.shape}, y shape: {y.shape}')\n","  a = (preds - y).abs().mean()\n","  print(f'a.grad_fn: {a.grad_fn}')\n","  # print(f'a: {a}')\n","  return a\n","\n","# loss(predictions(train_x, w, b), train_y)\n","\n","def apply_step(i, x, y, w, b, lr):\n","  preds = predictions(x, w, b)\n","  # print(f'preds.grad_fn: {preds.grad_fn}')\n","  # l = preds.backward()\n","  l = loss(preds, y)\n","  # print(f'l.grad_fn: {l.grad_fn}')\n","  l.backward()\n","  # print(f'w after backward: {w[0:1]}')\n","  print(f'w grads after backward: {w.grad.shape}, {w.grad[0:7, 0:3]}')\n","  # print(f'w.grad.data: {w.grad.data[0:3]}')\n","  # print(f'before b.data: {b.data}')\n","  # print(f'before b.grad: {b.grad.data}')\n","  w.data = w.data - (w.grad.data * lr)\n","  b.data = b.data - (b.grad.data * lr)\n","  print(f'after b.data: {b.data}')\n","  w.grad = b.grad = None\n","  return (w, b, l)\n","\n","def apply_steps(n, x, y, w, b, lr):\n","  results = []\n","  for i in range(n):\n","    # print(f'before b: {b[0:5]}')\n","    (w, b, l) = apply_step(i, x, y, w, b, lr)\n","    results.append(l)\n","    if(i % 1 == 0): print(f'loss at step({i}): {l}')\n","    # print(f'after b: {b[0:5]}')\n","  return (w, b, results)"],"metadata":{"id":"rJb62k2lNh6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["g = torch.Generator()\n","g.manual_seed(5)\n","w = torch.randn((784, 1), generator=g).to(device).requires_grad_(True)\n","# print(f'w: {w[0:1, 0:1]}')\n","# print(f'w cuda? {w.is_cuda}')\n","# print(f'w shape: {w.shape}, w: {w[0:4, 0:4]}')\n","b = torch.randn(12396, generator=g).to(device).requires_grad_(True)\n","# print(f'b cuda? {b.is_cuda}')\n","# print(f'len(b): {len(b)}, b: {b[0:9]}')\n","\n","# lrs = [0.001, 0.1, 1, 100, 10000, 100000]\n","lrs = [1]\n","for lr in lrs:\n","  print(f'lr: {lr}')\n","  fw, fb, fr = apply_steps(1, train_x, train_y, w, b, lr)"],"metadata":{"id":"1VYnI4-KWeDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#len(range(30)), len(results)\n","#plt.scatter(range(30), to_np(fr))\n","p = predictions(train_x, fw, fb)\n","type(p), len(p), p.shape\n","diff = (p <= 0.5).float().sum()\n","# type(diff), len(diff), diff.shape, diff\n","# diff, len(train_y), loss(p, train_y)\n","\n","# p[0], train_y[0], p[0] - train_y[0]\n","diffs = torch.nonzero((p < 0.5).float().unbind(1)[0])\n","trx = [Image.open(f) for f in (path/'train'/'3').ls().sorted()] + [Image.open(f) for f in (path/'train'/'7').ls().sorted()]\n","\n","# try = [Image.open(f) for f in (path/'train'/'7').ls().sorted()]\n","def images():\n","  for d in diffs[-10:-5]:\n","    print(f'd: {d}, prediction: {p[d]}')\n","    show_image(trx[d])\n","\n","  for i in range(5,10):\n","    print(f'i: {i}, prediction: {p[i]}')\n","    show_image(trx[i])"],"metadata":{"id":"OGqdsW4SQGJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lPteGaDokiXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#??torch.nonzero\n","t = tensor([0,1,0,2,2,3,0,1,1,1,1])\n","print(f'nonzero: {torch.nonzero(t)}')\n","\n","#??torch.linspace\n","a = torch.linspace(1, 9, steps=9).unsqueeze(1)\n","a, a[0], a[1], a[0][0],a[0:3]\n","a = np.array([[1, 2, 3, 4],\n","              [5, 6, 7, 8],\n","              [9, 10, 11, 12],\n","              [13, 14, 15, 16]])\n","a[0:3][0:3][0:3], a[0:3, 0:3]\n","a.sum(), a.sum(0), a.sum(1)"],"metadata":{"id":"pz13tprtyrmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_x), train_x.shape, type(train_x), len(train_x[0]), '\\n', train_x[0:3, 0:7])\n","print(len(train_y), train_y.shape, type(train_y), len(train_y[0]), train_y[0:3])\n","print(len(w), w.shape, type(w), len(w[0]), w[0:2, 0:3])\n","len(b), type(b), b[0:7]"],"metadata":{"id":"khozH93J9Bs-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.ones((4,1), requires_grad=True)\n","print(f'a: {a}')\n","print(f'a.grad_fn: {a.grad_fn}')\n","b = a + 1\n","print(f'b.grad_fn: {b.grad_fn}')\n","c = a.sum(1)\n","print(f'c.grad_fn: {c.grad_fn}')\n","print(f'c: {c}')\n","d = c.unsqueeze(1)\n","print(f'd.grad_fn: {d.grad_fn}')\n","print(f'd: {d}')"],"metadata":{"id":"LzlJN9-Pa1kG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x.shape, w.shape"],"metadata":{"id":"ESf0JXtsxA_Y"},"execution_count":null,"outputs":[]}]}